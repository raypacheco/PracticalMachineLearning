##Practical Machine Learning: Human Activity Recognition

###Introduction
Devices such as the Fitbit, Jawbone Up, and Nike FuelBank now make it possible to collect a large amount of data about personal activity. Currently, however, these devices quantify only how much of a particular activity is done, but rarely how well the activity is done.

This analysis uses data gathered by Velloso, E., et. al. in their research: [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/har). In this study, six participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). 

For the purposes of our project, we will explore using Random Forests to predict each class based on several of the variables in the dataset.

###Data Exploration and Cleaning

We need to load first load and subset the data. We will use the Caret package for this analysis.

```{r, warning=FALSE}
library(caret)

train <- read.csv("pml-training.csv", header=TRUE)
## Earlier exploration shows the variable new_window with "yes" contain only summary statistics.
train <- train[train$new_window == "no",]

summary(train)
```

We see several columns that are fully NA. These appear to be the summary statistics variables. We can remove all these by finding the near zero variance predictor columns. This will also remove any other columns with few unique values that would also be poor predictors.

```{r}
nzv <- nearZeroVar(train, saveMetrics = TRUE)

keep <- row.names(nzv[nzv[,"zeroVar"] == FALSE,])

train <- train[,keep]

names(train)
```

We are now down to 59 variables from the original 160. The first 6 columns only contain subject id and timestamps. Since these should be tied directly to each subject, we can remove these, as they would no make for good predictors. This will put us down to 53 variables.

```{r}
train <- train[,-c(1:6)]
```

###Fitting the Model

Now that the data has been processed we can proceed to splitting the data into two sets.

```{r}

set.seed(1985)

inTrain <- createDataPartition(train$classe, p=0.7, list=FALSE)

training <- train[inTrain,]

testing <- train[-inTrain,]

```

We can now fit our Random Forest model to the training set using cross-validation with 10 folds.

```{r, cache=TRUE}

ctrl <- trainControl(method="cv", number=10, allowParallel = TRUE)

set.seed(1234)
modelFit <- train(classe ~ ., data = training, method = "rf", trControl = ctrl)

modelFit$finalModel
```

###Results

The confusion matrix shows very low classification error, only a 0.7% out of bag error rate. We can now apply our model to the test set and get our out of sample error.

```{r}
prediction <- predict(modelFit, newdata = testing)

(sum(prediction != testing$classe)/nrow(testing))*100 ## out of sample error calculation

confusionMatrix(prediction, testing$classe)

```

We can see that the out of sample error rate is only 0.68%, which is good enough for our purposes. Looking at the confusion matrix we see that the model had the most problems labeling class D (lowering the dumbbell halfway). It put a few instances as class C (raising the dumbbell halfway). This makes sense as the motions are similar, with the biggest differences being the direction (up or down) of the motion.

In addition to our test dataset, a 20 row dataset was also provided for this project. When the model was applied to this small dataset and submitted for grading, it predicted the classe with 100% accuracy. The code is not provided, but the dataset can be found in the GitHub repo.

###Conclusion

For the purposes of this report we used a very simplistic approach with the Random Forest learning method, using only a training and test set with 10 fold cross-validation. To improve our accuracy we may attempt to use a more robust model validation technique. Combining predictors may also be something to explore in future studies.
